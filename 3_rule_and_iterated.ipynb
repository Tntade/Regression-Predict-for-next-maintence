{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>目前,模型和规则中,最后一批人的训练速度非常慢,且容易出现OOM情况<br>1.取消所有operation操作;<br>2.多使用persist和broadcast;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,time,gc,re\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext,SparkConf,SQLContext\n",
    "from pyspark.sql import Window\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.functions import mean,stddev,log\n",
    "import pyspark.sql.functions as fn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-08\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName('LQ_SKD_D333').config('spark.shuffle.consolidateFiles','true').getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "today=datetime.strftime(datetime.now(),'%Y-%m-%d')\n",
    "print(today)\n",
    "\n",
    "revise_types=['claim','routine_maintenance','accessory','accident_repair','general_repair',\n",
    "       'first_maintance','service_act']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_miss_cols(dff):\n",
    "    tmp=dff.agg(*[(1-(fn.count(c)/fn.count('*'))).alias(c) for c in dff.columns])\n",
    "    cols=tmp.columns\n",
    "    leng=len(cols)\n",
    "    lst=tmp.collect()\n",
    "    miss_cols=[]\n",
    "    for i in range(leng):\n",
    "        v=[x[i] for x in lst][0]\n",
    "        if v>0:\n",
    "            miss_cols.append(cols[i])\n",
    "    return miss_cols\n",
    "\n",
    "def check_df(df):\n",
    "    print(df.count())\n",
    "    print(df.select('vin','repair_date').dropDuplicates().count())\n",
    "    print(df.select('vin').distinct().count())\n",
    "    \n",
    "def check_ziduan(df1):\n",
    "    cols=df1.columns\n",
    "    cols.remove('vin')\n",
    "    for x in cols:\n",
    "        print(x)\n",
    "        print([y[0] for y in df1.select(x).distinct().collect()][:10])\n",
    "        print('-'*30)\n",
    "\n",
    "def get_user(df1):\n",
    "    df1=df1.withColumn('car_body_type',substring('vin',4,1)).\\\n",
    "    withColumn('gearbox',substring('vin',5,1)).withColumn('crew_protection_system',substring('vin',6,1)).\\\n",
    "    withColumn('car_class',substring('vin',7,2)).withColumn('output_year',substring('vin',10,1)).\\\n",
    "    withColumn('assembly_factory',substring('vin',11,1))\n",
    "\n",
    "    df1=df1.withColumn('user_tag',concat_ws(',',col('car_body_type'),col('gearbox'),col('crew_protection_system'),col('car_class'),col('output_year'),col('assembly_factory')))\n",
    "    return df1\n",
    "\n",
    "def check_time(s1):\n",
    "    print('run time is:',(time.time()-s1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fenlei(dff,today):\n",
    "    \n",
    "    dff=dff.withColumn('purchase_date',to_date(col('purchase_date'))).\\\n",
    "    withColumn('repair_date',to_date(col('repair_date'))).\\\n",
    "    withColumn('today',to_date(lit(today))).\\\n",
    "    withColumn('car_age',datediff(col('today'),col('purchase_date'))).\\\n",
    "    withColumn('car_age_bin',ceil(col('car_age')/180))\n",
    "    \n",
    "    if 'changgui_times' in dff.columns:\n",
    "        dff=dff.drop('changgui_times')\n",
    "        \n",
    "    tmp=dff.filter(col('first_maintance')!=1)\n",
    "    tmp=tmp.filter(col('routine_maintenance')==1).select('vin','repair_date').dropDuplicates().groupBy('vin').agg(count('repair_date').alias('changgui_times'))\n",
    "    dff=dff.join(tmp,on='vin',how='left')\n",
    "    dff=dff.withColumn('changgui_times',when(col('changgui_times').isNull(),lit(0)).otherwise(col('changgui_times')))\n",
    "    \n",
    "    if 'shoubao_times' in dff.columns:\n",
    "        dff=dff.drop('shoubao_times')\n",
    "    \n",
    "    tmp=dff.filter(col('first_maintance')==1).select('vin').distinct().withColumn('shoubao_times',lit(1))\n",
    "    dff=dff.join(tmp,on='vin',how='left')\n",
    "    dff=dff.withColumn('shoubao_times',when(col('shoubao_times').isNull(),lit(0)).otherwise(col('shoubao_times')))\n",
    "    \n",
    "    dff=dff.withColumn('the_group',when((col('shoubao_times')==1)&(col('changgui_times')==0),lit('有首保,无常规,预测第1次常规')).\\\n",
    "                       when((col('shoubao_times')==1)&(col('changgui_times')==1),lit('有首保,1常规,预测第2次常规')).\\\n",
    "                       when((col('shoubao_times')==1)&(col('changgui_times')==2),lit('有首保,2次常规,预测第3次常规')).\\\n",
    "                       when((col('shoubao_times')==1)&(col('changgui_times')>=3),lit('3次及以上常规,预测接下去常规')).\\\n",
    "                       when((col('shoubao_times')==0)&(col('changgui_times')==1),lit('无首保,1次常规,预测第2次常规(购车)')).\\\n",
    "                       when((col('shoubao_times')==0)&(col('changgui_times')==2),lit('无首保,2次常规,预测第3次常规')).\\\n",
    "                       when((col('shoubao_times')==0)&(col('changgui_times')>=3),lit('3次及以上常规,预测接下去常规')).\\\n",
    "                       when((col('shoubao_times')==0)&(col('changgui_times')==0)&(col('car_age_bin')<=3),lit('无保养记录,预测首保')).\\\n",
    "                       when((col('shoubao_times')==0)&(col('changgui_times')==0)&(col('car_age_bin')>3),lit('无保养习惯')).\\\n",
    "                       otherwise(lit('错误提示:没有被分配到人群!')))\n",
    "\n",
    "    print('查看是否有没有被分配到的人群')\n",
    "    c=dff.select('vin','the_group').dropDuplicates()\n",
    "    print(c.count())\n",
    "    print(c.select('vin').distinct().count())\n",
    "    mis=get_miss_cols(c)\n",
    "    print(mis)\n",
    "    \n",
    "    print('查看对应的每一类人群的VIN数量')\n",
    "    tmp=dff.select('vin','the_group').dropDuplicates().groupBy('the_group').agg(count('vin').alias('cnt'))\n",
    "    print(tmp.show())\n",
    "    \n",
    "    baoyangdf1=dff.filter((col('first_maintance')==1)|(col('routine_maintenance')==1))\n",
    "    baoyang_vin=baoyangdf1.select('vin').distinct().dropDuplicates().withColumn('have_baoyang',lit(1))\n",
    "    \n",
    "    nobaoyang1=dff.join(baoyang_vin,on='vin',how='left').withColumn('have_baoyang',when(col('have_baoyang').isNull(),lit(0)).otherwise(col('have_baoyang')))\n",
    "    \n",
    "    return baoyangdf1,nobaoyang1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_run(df):\n",
    "    df=df.withColumn('purchase_date',to_date(col('purchase_date'))).withColumn('repair_date',to_date(col('repair_date')))\n",
    "    df=df.withColumn('last_repair_date',lag(col('repair_date'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    df=df.withColumn('last_mile',lag(col('mile'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    df=df.withColumn('last_repair_date',when(col('last_repair_date').isNull(),col('purchase_date')).otherwise(col('last_repair_date'))).\\\n",
    "    withColumn('last_mile',when(col('last_mile').isNull(),lit(0)).otherwise(col('last_mile'))).\\\n",
    "    withColumn('daydiff',datediff(col('repair_date'),col('last_repair_date'))).\\\n",
    "    withColumn('milediff',col('mile')-col('last_mile')).\\\n",
    "    withColumn('dayofmile',col('milediff')/col('daydiff'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_rule(rule):\n",
    "    \n",
    "#     print('历史保养次数')\n",
    "    if 'baoyang_times' in rule.columns:\n",
    "        rule=rule.drop('baoyang_times')\n",
    "    tmp=rule.select('vin','repair_date').dropDuplicates().groupBy('vin').agg(count('repair_date').alias('baoyang_times'))\n",
    "    rule=rule.join(tmp,on=['vin'],how='left')\n",
    "    \n",
    "#     print('得到对应的上一次修理日期和公里数')\n",
    "    rule=rule.withColumn('last_repair_date',lag(col('repair_date'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    rule=rule.withColumn('last_mile',lag(col('mile'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    rule=rule.withColumn('last_repair_date',when(col('last_repair_date').isNull(),col('purchase_date')).otherwise(col('last_repair_date')))\n",
    "    rule=rule.withColumn('last_mile',when(col('last_mile').isNull(),lit(0)).otherwise(col('last_mile')))\n",
    "    rule=rule.withColumn('daydiff',datediff(col('repair_date'),col('last_repair_date')))\n",
    "    rule=rule.withColumn('milediff',col('mile')-col('last_mile'))\n",
    "    rule=rule.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    rule=broadcast(rule)\n",
    "    \n",
    "#     print('开始天数差trick清洗')\n",
    "    rule1=rule.filter(col('daydiff')>7.0)\n",
    "    rule2=rule1.drop('baoyang_times')\n",
    "    \n",
    "    tmp=rule2.select('vin','repair_date').dropDuplicates().groupBy('vin').agg(count('repair_date').alias('baoyang_times'))\n",
    "    rule2=rule2.join(tmp,on=['vin'],how='left')\n",
    "    \n",
    "    df11=rule2.filter(col('baoyang_times')==1)\n",
    "    df11=df11.filter(col('daydiff')>7)\n",
    "    df22=rule2.filter(col('baoyang_times')==2)\n",
    "    tmp=df22.select('vin','repair_date','daydiff').dropDuplicates().groupBy('vin').agg(mean('daydiff')).withColumnRenamed('avg(daydiff)','daydiff_habit')\n",
    "    df22=df22.join(tmp,on=['vin'],how='left').withColumn('right_habit',col('daydiff_habit')*0.5)\n",
    "    df22=df22.filter(col('daydiff')>=col('right_habit'))\n",
    "    df33=rule2.filter(col('baoyang_times')>=3)\n",
    "    df33=df33.filter(col('daydiff')>14)\n",
    "\n",
    "    df11=df11.drop('baoyang_times')\n",
    "    df22=df22.drop('right_habit','daydiff_habit','baoyang_times')\n",
    "    df33=df33.drop('baoyang_times')\n",
    "    \n",
    "    rule2=df11.unionAll(df22).unionAll(df33)\n",
    "    if 'in_model' in rule2.columns:\n",
    "        rule2=rule2.drop('in_model')\n",
    "    rule2=rule2.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    rule2=broadcast(rule2)\n",
    "    \n",
    "    del df11\n",
    "    del df22\n",
    "    del df33\n",
    "    del rule1\n",
    "    del tmp\n",
    "    \n",
    "    rule=rule.join(rule2.select('vin').distinct().withColumn('cleared_vin',lit(0)),on='vin',how='left').\\\n",
    "    withColumn('cleared_vin',when(col('cleared_vin').isNull(),lit(1)).otherwise(col('cleared_vin')))\n",
    "    \n",
    "    cleared=rule.filter(col('cleared_vin')==1)\n",
    "    if 'in_model' in cleared.columns:\n",
    "        cleared=cleared.drop('in_model')\n",
    "    if 'cleared_vin' in cleared.columns:\n",
    "        cleared=cleared.drop('cleared_vin')\n",
    "    cleared=cleared.withColumn('first_maintance',lit(0)).withColumn('routine_maintenance',lit(0))\n",
    "    cleared=cleared.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    cleared=broadcast(cleared)\n",
    "    \n",
    "    del rule\n",
    "\n",
    "    return rule2,cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_rule_further(rule2):\n",
    "    if 'baoyang_times' in rule2.columns:\n",
    "        rule2=rule2.drop('baoyang_times')\n",
    "    if 'last_repair_date' in rule2.columns:\n",
    "        rule2=rule2.drop('last_repair_date')\n",
    "    if 'last_mile' in rule2.columns:\n",
    "        rule2=rule2.drop('last_mile')\n",
    "    if 'daydiff' in rule2.columns:\n",
    "        rule2=rule2.drop('daydiff')\n",
    "    if 'milediff' in rule2.columns:\n",
    "        rule2=rule2.drop('milediff')\n",
    "    if 'dayofmile' in rule2.columns:\n",
    "        rule2=rule2.drop('dayofmile')\n",
    "    if 'baoyang_id' in rule2.columns:\n",
    "        rule2=rule2.drop('baoyang_id')\n",
    "    \n",
    "#     print('清洗完成,重新跑last_run')\n",
    "    tmp=rule2.select('vin','repair_date').dropDuplicates().groupBy('vin').agg(count('repair_date').alias('baoyang_times'))\n",
    "    rule2=rule2.join(tmp,on=['vin'],how='left')\n",
    "\n",
    "    rule2=rule2.withColumn('last_repair_date',lag(col('repair_date'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    rule2=rule2.withColumn('last_mile',lag(col('mile'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    rule2=rule2.withColumn('last_repair_date',when(col('last_repair_date').isNull(),col('purchase_date')).otherwise(col('last_repair_date')))\n",
    "    rule2=rule2.withColumn('last_mile',when(col('last_mile').isNull(),lit(0)).otherwise(col('last_mile')))\n",
    "    rule2=rule2.withColumn('daydiff',datediff(col('repair_date'),col('last_repair_date')))\n",
    "    rule2=rule2.withColumn('milediff',col('mile')-col('last_mile'))\n",
    "    rule2=rule2.withColumn('dayofmile',col('milediff')/col('daydiff'))\n",
    "    rule2=rule2.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    rule3=rule2.withColumn('in_model',lit(0)).dropDuplicates()\n",
    "    rule3=rule3.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "    rule3=broadcast(rule3)\n",
    "    \n",
    "    del tmp\n",
    "    del rule2\n",
    "\n",
    "    return rule3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165067\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql('select * from clms.skd_final_zhudan')\n",
    "print(df.select('vin').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=df.filter(col('mark')=='模型预测')\n",
    "# print(model.count())\n",
    "model=model.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "model=broadcast(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule=df.filter(col('mark')=='规则预测')\n",
    "# print(rule.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobaoyang=df.filter(col('mark')=='非保养数据')\n",
    "# print(nobaoyang.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.count()==model.count()+rule.count()+nobaoyang.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "baoyang_vin=model.select('vin').distinct().unionAll(rule.select('vin').distinct())\n",
    "baoyang_vin=baoyang_vin.withColumn('have_baoyang',lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'have_baoyang' in nobaoyang.columns:\n",
    "    nobaoyang=nobaoyang.drop('have_baoyang')\n",
    "nobaoyang=nobaoyang.join(baoyang_vin,on=['vin'],how='left')\n",
    "nobaoyang=nobaoyang.withColumn('have_baoyang',when(col('have_baoyang').isNull(),lit(0)).otherwise(col('have_baoyang')))\n",
    "nobaoyang=nobaoyang.filter(col('have_baoyang')==0)\n",
    "nobaoyang=nobaoyang.drop('have_baoyang')\n",
    "# print(nobaoyang.count())\n",
    "# print(nobaoyang.select('vin').distinct().count())\n",
    "\n",
    "nobaoyang=nobaoyang.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "nobaoyang=broadcast(nobaoyang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule2,cleared=clear_rule(rule)\n",
    "rule2=rule2.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rule2=broadcast(rule2)\n",
    "\n",
    "#被天数清洗掉的vin,默认为没有保养数据,将常规保养与首保全都置0\n",
    "cleared=cleared.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "cleared=broadcast(cleared)\n",
    "\n",
    "rule3=clear_rule_further(rule2)\n",
    "rule3=rule3.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rule3=broadcast(rule3)\n",
    "\n",
    "del rule2\n",
    "del rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vin', 'repair_date', 'mile', 'repair_amount', 'repair_amount_pre_discount', 'labor_fee', 'part_fee', 'claim', 'routine_maintenance', 'accessory', 'accident_repair', 'general_repair', 'first_maintance', 'service_act', 'belong_dealer_code', 'city', 'customer_type', 'purchase_date', 'ies_name', 'province', 'family_name', 'city_class', 'member_type', 'whether_bind_wechat', 'car_body_type', 'gearbox', 'crew_protection_system', 'car_class', 'output_year', 'assembly_factory', 'user_tag', 'repair_type', 'mark', 'gender_code', 'member_status', 'member_age']\n"
     ]
    }
   ],
   "source": [
    "same_cols=[x for x in model.columns if x in [y for y in rule3.columns if y in cleared.columns]]\n",
    "same_cols=[x for x in same_cols if x in nobaoyang.columns]\n",
    "print(same_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重置mark\n",
    "model=model.withColumn('mark',lit('model'))\n",
    "rule3=rule3.withColumn('mark',lit('rule'))\n",
    "cleared=cleared.withColumn('mark',lit('nobaoyang_cleared'))\n",
    "nobaoyang=nobaoyang.withColumn('mark',lit('nobaoyang'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=model.select(same_cols).unionAll(rule3.select(same_cols)).unionAll(cleared.select(same_cols)).unionAll(nobaoyang.select(same_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543561\n",
      "165067\n"
     ]
    }
   ],
   "source": [
    "dff=dff.withColumn('purchase_date',to_date(col('purchase_date'))).\\\n",
    "withColumn('repair_date',to_date(col('repair_date'))).\\\n",
    "withColumn('repair_day',datediff(col('repair_date'),col('purchase_date')))\n",
    "\n",
    "dff=dff.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "dff=broadcast(dff)\n",
    "\n",
    "print(dff.count())\n",
    "print(dff.select('vin').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206870\n",
      "206870\n"
     ]
    }
   ],
   "source": [
    "veh=spark.sql('select * from clms.skd_vehicle')\n",
    "print(veh.count())\n",
    "print(veh.select('vin').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purchase_date\n"
     ]
    }
   ],
   "source": [
    "mis1,mis2=get_miss_cols(veh)\n",
    "print(mis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'重构全量数据'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"重构全量数据\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh=veh.withColumnRenamed('purchase_date','purchase_date_in_veh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vin', 'belong_dealer_code', 'city', 'customer_type', 'ies_name', 'province', 'family_name', 'city_class', 'member_type', 'whether_bind_wechat', 'car_body_type', 'gearbox', 'crew_protection_system', 'car_class', 'output_year', 'assembly_factory', 'user_tag', 'gender_code', 'member_status', 'member_age']\n"
     ]
    }
   ],
   "source": [
    "same=[x for x in dff.columns if x in veh.columns]\n",
    "print(same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_1=dff.drop('belong_dealer_code', 'city', 'customer_type', 'ies_name', 'province', 'family_name', 'city_class', 'member_type', 'whether_bind_wechat', 'car_body_type', 'gearbox', 'crew_protection_system', 'car_class', 'output_year', 'assembly_factory', 'user_tag', 'gender_code', 'member_status', 'member_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206870\n"
     ]
    }
   ],
   "source": [
    "all_df=veh.join(dff_1,on=['vin'],how='left')\n",
    "print(all_df.select('vin').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "165067\n"
     ]
    }
   ],
   "source": [
    "all_df=all_df.withColumn('purchase_date',when(col('purchase_date').isNull(),col('purchase_date_in_veh')).otherwise(col('purchase_date')))\n",
    "all_df=all_df.filter(col('purchase_date').isNotNull())\n",
    "check=all_df.filter(col('purchase_date')>col('repair_date'))\n",
    "print(check.count())\n",
    "all_df=all_df.filter(col('purchase_date')<=col('repair_date'))\n",
    "all_df=all_df.withColumn('repair_date',when(col('repair_date').isNull(),col('purchase_date')).otherwise(col('repair_date')))\n",
    "all_df=all_df.withColumn('mile',when(col('mile').isNull(),lit(0)).otherwise(col('mile')))\n",
    "print(all_df.select('vin').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看是否有没有被分配到的人群\n",
      "165067\n",
      "165067\n",
      "[]\n",
      "查看对应的每一类人群的VIN数量\n",
      "+--------------------+-----+\n",
      "|           the_group|  cnt|\n",
      "+--------------------+-----+\n",
      "|     有首保,1常规,预测第2次常规|14978|\n",
      "|    有首保,2次常规,预测第3次常规|12528|\n",
      "|     有首保,无常规,预测第1次常规|37619|\n",
      "|    无首保,2次常规,预测第3次常规| 3881|\n",
      "|     3次及以上常规,预测接下去常规|58240|\n",
      "|               无保养习惯|17257|\n",
      "|          无保养记录,预测首保|  615|\n",
      "|无首保,1次常规,预测第2次常规(购车)|19949|\n",
      "+--------------------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print('开始对人群进行分类')\n",
    "baoyangdf,nobaoyang=fenlei(all_df,today)\n",
    "# print(baoyangdf.count())\n",
    "# print(nobaoyang.count())\n",
    "\n",
    "baoyangdf=baoyangdf.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "baoyangdf=broadcast(baoyangdf)\n",
    "\n",
    "nobaoyang=nobaoyang.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "nobaoyang=broadcast(nobaoyang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date3(x,y,z):\n",
    "    a = x + relativedelta(days = int(y)) + relativedelta(days=int(z))\n",
    "    return a\n",
    "\n",
    "def get_date2(x,y):\n",
    "    a = x + relativedelta(days = int(y))\n",
    "    return a\n",
    "\n",
    "get_dates_udf3 = udf(lambda x,y,z:get_date3(x,y,z),DateType())\n",
    "get_dates_udf2 = udf(lambda x,y:get_date2(x,y),DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_date_mile(data3,end_id,daydiff_median,milediff_median,dayofmile_median):\n",
    "    \"\"\"milediff_pred代表的是公里数差,milediff_%end_id代表的最后一个公里数差,next_milediff代表的是下一次进店间隔的趋势值\"\"\"\n",
    "    \n",
    "    if 'milediff_pred' not in data3.columns:\n",
    "        data3=data3.withColumn('milediff_pred',col('milediff_%d'%end_id)*col('next_milediff'))\n",
    "    if 'dayofmile_pred' not in data3.columns:\n",
    "        data3=data3.withColumn('dayofmile_pred',col('dayofmile_%d'%end_id)*col('next_dayofmile'))\n",
    "    \n",
    "#     print('检查脏数据情况')\n",
    "#     print('预测出来的公里数差<=0',data3.filter(col('milediff_pred')<=0).select('vin').distinct().count())\n",
    "    \n",
    "    data3=data3.withColumn('milediff_pred',when(col('milediff_pred')<=0,lit(milediff_median)).otherwise(col('milediff_pred'))).\\\n",
    "    withColumn('dayofmile_pred',when(col('dayofmile_pred')<=0,lit(dayofmile_median)).otherwise(col('dayofmile_pred')))\n",
    "    \n",
    "    data3=data3.withColumn('next_mile',col('mile_%d'%end_id)+col('milediff_pred')).\\\n",
    "    withColumn('next_mile',ceil('next_mile'))\n",
    "    \n",
    "    if 'daydiff_pred' in data3.columns:\n",
    "        data3=data3.drop('daydiff_pred')\n",
    "        \n",
    "    data3=data3.withColumn('daydiff_pred',col('milediff_pred')/(col('dayofmile_pred')+0.001)).\\\n",
    "    withColumn('daydiff_pred',ceil('daydiff_pred'))\n",
    "    \n",
    "#     print('检查脏数据')\n",
    "#     print('预测出来的天数差<=0',data3.filter(col('daydiff_pred')<=0).select('vin').distinct().count())\n",
    "    data3=data3.withColumn('daydiff_pred',when(col('daydiff_pred').isNull(),lit(daydiff_median)).when(col('daydiff_pred')<=0,lit(daydiff_median)).otherwise(col('daydiff_pred')))\n",
    "    data3=data3.withColumn('next_date',get_dates_udf3(col('purchase_date'),col('repair_day_%d'%end_id),col('daydiff_pred')))\n",
    "    \n",
    "    \"\"\"其中last_baoyang_date代表是最后一次进店保养的时间\"\"\"\n",
    "    data3=data3.withColumn('last_baoyang_date',get_dates_udf2(col('purchase_date'),col('repair_day_%d'%end_id))).\\\n",
    "    withColumn('last_baoyang_mile',col('mile_%d'%end_id))\n",
    "    \n",
    "    return data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群0:预测首保"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_0(nobaoyang):\n",
    "    rule0=nobaoyang.filter(col('the_group')=='无保养记录,预测首保').\\\n",
    "    withColumn('repair_day_0',lit(0)).\\\n",
    "    withColumn('mile_0',lit(0))\n",
    "    return rule0\n",
    "\n",
    "# print('这一个人群比较特殊,购车到首保的天数差,不能直接迭代,因此需要再拟合一个可以往下迭代的数据')\n",
    "def get_trend_0(baoyangdf):\n",
    "    \"\"\"趋势中去掉了脏数据,但是原始自身数据中脏数据还是存在,所以当最终预测出来出现负数的情况下,可能第一考虑的就是原始数据中就有脏数据\"\"\"\n",
    "    df0=baoyangdf.filter(col('first_maintance')==1)\n",
    "    df0=get_last_run(df0)\n",
    "    df0=df0.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    df00=df0.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=df0.groupBy('vin').pivot('baoyang_id',[1]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        df00=df00.join(tmp,on='vin',how='left')\n",
    "    df00=df00.filter((col('daydiff_1')>0)&(col('milediff_1')>0))\n",
    "    return df00\n",
    "\n",
    "def nihe_0(df00):\n",
    "    \n",
    "    df00=df00.toPandas()\n",
    "    \n",
    "    daydiff_median=df00['daydiff_1'].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_1'].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_1'].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['belong_dealer_code']\n",
    "    nihe0=df00.groupby(key0)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe0.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe0.columns=['first_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField('belong_dealer_code', StringType(), True),\n",
    "                        StructField(\"first_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"first_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"first_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "\n",
    "    key1=['user_tag']\n",
    "    nihe1=df00.groupby(key1)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe1.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe1.columns=['second_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe1=nihe1.reset_index()\n",
    "    schema1=StructType([StructField('user_tag', StringType(), True),\n",
    "                        StructField(\"second_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"second_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"second_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe1=spark.createDataFrame(nihe1,schema=schema1)\n",
    "    \n",
    "    key2=['ies_name']\n",
    "    nihe2=df00.groupby(key2)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe2.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe2.columns=['third_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe2=nihe2.reset_index()\n",
    "    schema2=StructType([StructField('ies_name', StringType(), True),\n",
    "                        StructField(\"third_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"third_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"third_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe2=spark.createDataFrame(nihe2,schema=schema2)\n",
    "\n",
    "    return key0,key1,key2,nihe0,nihe1,nihe2,daydiff_median,milediff_median,dayofmile_median\n",
    "\n",
    "\n",
    "def get_trend_00(baoyangdf):\n",
    "    #取出用户第一次常规到首保的天数差/公里数差\n",
    "    shoubao_vin=baoyangdf.filter(col('first_maintance')==1).select('vin').distinct().withColumn('have_shoubao',lit(1))\n",
    "    df0=baoyangdf.join(shoubao_vin,on=['vin'],how='left').\\\n",
    "    withColumn('have_shoubao',when(col('have_shoubao').isNull(),lit(0)).otherwise(col('have_shoubao')))\n",
    "    df0=df0.filter(col('have_shoubao')==1)\n",
    "    df0=df0.withColumn('day_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    df0=df0.filter(col('day_id')<=2).drop('day_id')\n",
    "    if 'baoyang_id' in df0.columns:\n",
    "        df0=df0.drop('baoyang_id')\n",
    "    df0=df0.withColumn('purchase_date',to_date(col('purchase_date'))).\\\n",
    "    withColumn('repair_date',to_date(col('repair_date')))\n",
    "    df0=df0.withColumn('last_repair_date',lag(col('repair_date'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    df0=df0.withColumn('last_mile',lag(col('mile'),-1).over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    df0=df0.filter(col('last_repair_date').isNotNull())\n",
    "    df0=df0.withColumn('daydiff',datediff(col('repair_date'),col('last_repair_date'))).\\\n",
    "    withColumn('milediff',col('mile')-col('last_mile')).\\\n",
    "    withColumn('dayofmile',col('milediff')/col('daydiff'))\n",
    "    df0=df0.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "#     print(df0.count())\n",
    "#     print(df0.select('vin').distinct().count())\n",
    "    \n",
    "    df00=df0.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=df0.groupBy('vin').pivot('baoyang_id',[1]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        df00=df00.join(tmp,on='vin',how='left')\n",
    "    df00=df00.filter((col('daydiff_1')>0)&(col('milediff_1')>0))\n",
    "    \n",
    "    return df00\n",
    "\n",
    "\n",
    "def nihe_00(df00):\n",
    "    \n",
    "    df00=df00.toPandas()\n",
    "    \n",
    "    daydiff_median=df00['daydiff_1'].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_1'].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_1'].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['belong_dealer_code']\n",
    "    df00['belong_dealer_code']=df00['belong_dealer_code'].astype(str)\n",
    "    nihe0=df00.groupby(key0)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe0.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe0.columns=['changgui_first_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField('belong_dealer_code', StringType(), True),\n",
    "                        StructField(\"changgui_first_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"changgui_first_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"changgui_first_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "\n",
    "    key1=['user_tag']\n",
    "    df00['user_tag']=df00['user_tag'].astype(str)\n",
    "    nihe1=df00.groupby(key1)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe1.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe1.columns=['changgui_second_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe1=nihe1.reset_index()\n",
    "    schema1=StructType([StructField('user_tag', StringType(), True),\n",
    "                        StructField(\"changgui_second_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"changgui_second_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"changgui_second_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe1=spark.createDataFrame(nihe1,schema=schema1)\n",
    "    \n",
    "    key2=['ies_name']\n",
    "    df00['ies_name']=df00['ies_name'].astype(str)\n",
    "    nihe2=df00.groupby(key2)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe2.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe2.columns=['changgui_third_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe2=nihe2.reset_index()\n",
    "    schema2=StructType([StructField('ies_name', StringType(), True),\n",
    "                        StructField(\"changgui_third_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"changgui_third_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"changgui_third_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe2=spark.createDataFrame(nihe2,schema=schema2)\n",
    "\n",
    "    return key0,key1,key2,nihe0,nihe1,nihe2,daydiff_median,milediff_median,dayofmile_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=blue>其中last_baoyang_date指的是最后一次保养数据,然后加上我们预测的接下去的保养天数差后,就得到了最新的next_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 3.3920891284942627\n"
     ]
    }
   ],
   "source": [
    "s1=time.time()\n",
    "rule0=get_rule_0(nobaoyang).dropDuplicates()\n",
    "trend0=get_trend_0(baoyangdf)\n",
    "key0,key1,key2,nihe0,nihe1,nihe2,daydiff_median,milediff_median,dayofmile_median=nihe_0(trend0)\n",
    "rule0=rule0.join(nihe0,on=key0,how='left').join(nihe1,on=key1,how='left').join(nihe2,on=key2,how='left')\n",
    "\n",
    "rule0=rule0.withColumn('daydiff_pred',col('first_daydiff_pred')).\\\n",
    "withColumn('daydiff_pred',when((col('daydiff_pred').isNull())&(col('second_daydiff_pred').isNotNull()),col('second_daydiff_pred')).when((col('daydiff_pred').isNull())&(col('third_daydiff_pred').isNotNull()),col('third_daydiff_pred')).otherwise(col('daydiff_pred'))).\\\n",
    "withColumn('milediff_pred',col('first_milediff_pred')).\\\n",
    "withColumn('milediff_pred',when((col('milediff_pred').isNull())&(col('second_milediff_pred').isNotNull()),col('second_milediff_pred')).when((col('milediff_pred').isNull())&(col('third_milediff_pred').isNotNull()),col('third_milediff_pred')).otherwise(col('milediff_pred'))).\\\n",
    "withColumn('dayofmile_pred',col('first_dayofmile_pred')).\\\n",
    "withColumn('dayofmile_pred',when((col('dayofmile_pred').isNull())&(col('second_dayofmile_pred').isNotNull()),col('second_dayofmile_pred')).when((col('dayofmile_pred').isNull())&(col('third_dayofmile_pred').isNotNull()),col('third_dayofmile_pred')).otherwise(col('dayofmile_pred')))\n",
    "\n",
    "trend00=get_trend_00(baoyangdf)\n",
    "\n",
    "key0,key1,key2,nihe0,nihe1,nihe2,daydiff_median,milediff_median,dayofmile_median=nihe_00(trend00)\n",
    "rule0=rule0.join(nihe0,on=key0,how='left').join(nihe1,on=key1,how='left').join(nihe2,on=key2,how='left')\n",
    "\n",
    "rule0=rule0.withColumn('changgui_daydiff_pred',col('changgui_first_daydiff_pred')).\\\n",
    "withColumn('changgui_daydiff_pred',when((col('changgui_daydiff_pred').isNull())&(col('changgui_second_daydiff_pred').isNotNull()),col('changgui_second_daydiff_pred')).when((col('changgui_daydiff_pred').isNull())&(col('changgui_third_daydiff_pred').isNotNull()),col('changgui_third_daydiff_pred')).otherwise(col('changgui_daydiff_pred'))).\\\n",
    "withColumn('changgui_milediff_pred',col('changgui_first_milediff_pred')).\\\n",
    "withColumn('changgui_milediff_pred',when((col('changgui_milediff_pred').isNull())&(col('changgui_second_milediff_pred').isNotNull()),col('changgui_second_milediff_pred')).when((col('changgui_milediff_pred').isNull())&(col('changgui_third_milediff_pred').isNotNull()),col('changgui_third_milediff_pred')).otherwise(col('changgui_milediff_pred'))).\\\n",
    "withColumn('changgui_dayofmile_pred',col('changgui_first_dayofmile_pred')).\\\n",
    "withColumn('changgui_dayofmile_pred',when((col('changgui_dayofmile_pred').isNull())&(col('changgui_second_dayofmile_pred').isNotNull()),col('changgui_second_dayofmile_pred')).when((col('changgui_dayofmile_pred').isNull())&(col('changgui_third_dayofmile_pred').isNotNull()),col('changgui_third_dayofmile_pred')).otherwise(col('changgui_dayofmile_pred')))\n",
    "\n",
    "end_id=0\n",
    "fin_rule0=get_next_date_mile(rule0,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule0=fin_rule0.withColumn('iterated_daydiff',col('changgui_daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('changgui_dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('changgui_milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('next_date')).\\\n",
    "withColumn('iterated_start_mile',col('next_mile'))\n",
    "\n",
    "need_cols=['vin', 'last_baoyang_date', 'last_baoyang_mile', 'daydiff_pred', 'milediff_pred',\n",
    "               'dayofmile_pred', 'next_date', 'next_mile', 'purchase_date',\n",
    "               'iterated_daydiff', 'iterated_start_date','iterated_milediff',\n",
    "               'iterated_start_mile', 'iterated_dayofmile', 'the_group']\n",
    "fin_rule0=fin_rule0.select(need_cols).dropDuplicates()\n",
    "\n",
    "fin_rule0=fin_rule0.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule0=broadcast(fin_rule0)\n",
    "\n",
    "# print(fin_rule0.count())\n",
    "# print(fin_rule0.select('vin').distinct().count())\n",
    "\n",
    "fin_rule0.toPandas().to_csv('rule0.csv',index=False,encoding='utf-8')\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群1:没有保养习惯的人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 1.3718855500221252\n"
     ]
    }
   ],
   "source": [
    "def get_rule_1(nobaoyang1):\n",
    "    rule1=nobaoyang1.filter(col('the_group')=='无保养习惯').\\\n",
    "    withColumn('repair_day_0',lit(0)).\\\n",
    "    withColumn('mile_0',lit(0))\n",
    "    return rule1\n",
    "\n",
    "def get_trend_1(baoyangdf1):\n",
    "#     print('#取出车龄>3的人群的,在车龄>3之后的第一次进店数据,根据车系喂入')\n",
    "    dff1=baoyangdf1.filter(col('car_age_bin')>3).withColumn('repair_date',to_date(col('repair_date'))).\\\n",
    "    withColumn('purchase_date',to_date(col('purchase_date'))).\\\n",
    "    withColumn('instore_age',datediff(col('repair_date'),col('purchase_date')))\n",
    "    \n",
    "    dff1=dff1.filter(col('instore_age')>540)\n",
    "    dff1=dff1.withColumn('instore_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    dff1=dff1.filter(col('instore_id')==1).drop('instore_id')\n",
    "    \n",
    "    if 'baoyang_id' in dff1.columns:\n",
    "        dff1=dff1.drop('baoyang_id')\n",
    "    \n",
    "    dff1=dff1.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    dff1=get_last_run(dff1)\n",
    "    \n",
    "    dff11=dff1.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=dff1.groupBy('vin').pivot('baoyang_id',[1]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        dff11=dff11.join(tmp,on='vin',how='left')\n",
    "    dff11=dff11.filter((col('daydiff_1')>0)&(col('milediff_1')>0))\n",
    "    return dff11\n",
    "\n",
    "\n",
    "def nihe_1(df00):\n",
    "    \n",
    "    df00=df00.toPandas()\n",
    "    \n",
    "    daydiff_median=df00['daydiff_1'].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_1'].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_1'].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['ies_name']\n",
    "    nihe0=df00.groupby(key0)['daydiff_1','milediff_1','dayofmile_1'].median()\n",
    "    nihe0.rename(columns={'daydiff_1':'daydiff_pred','milediff_1':'milediff_pred','dayofmile_1':'dayofmile_pred'},inplace=True)\n",
    "    nihe0.columns=['first_'+x for x in ['daydiff_pred','milediff_pred','dayofmile_pred']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField(key0[0], StringType(), True),\n",
    "                        StructField(\"first_daydiff_pred\", DoubleType(), True),\n",
    "                        StructField(\"first_milediff_pred\", DoubleType(), True),\n",
    "                        StructField(\"first_dayofmile_pred\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "\n",
    "    return key0,nihe0,daydiff_median,milediff_median,dayofmile_median\n",
    "\n",
    "s1=time.time()\n",
    "rule1=get_rule_1(nobaoyang).dropDuplicates()\n",
    "trend1=get_trend_1(baoyangdf)\n",
    "key0,nihe0,daydiff_median,milediff_median,dayofmile_median=nihe_1(trend1)\n",
    "rule1=rule1.join(nihe0,on=key0,how='left')\n",
    "\n",
    "rule1=rule1.withColumn('daydiff_pred',col('first_daydiff_pred')).\\\n",
    "withColumn('milediff_pred',col('first_milediff_pred')).\\\n",
    "withColumn('dayofmile_pred',col('first_dayofmile_pred')).\\\n",
    "withColumn('daydiff_pred',when(col('daydiff_pred').isNull(),lit(daydiff_median)).otherwise(col('daydiff_pred'))).\\\n",
    "withColumn('milediff_pred',when(col('milediff_pred').isNull(),lit(milediff_median)).otherwise(col('milediff_pred'))).\\\n",
    "withColumn('dayofmile_pred',when(col('dayofmile_pred').isNull(),lit(dayofmile_median)).otherwise(col('dayofmile_pred')))\n",
    "\n",
    "end_id=0\n",
    "fin_rule1=get_next_date_mile(rule1,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "#由于是无保养习惯人群,所以直接给的540天以后第一次进店后到购车时间的天数差,并以此为迭代数据,否则的话,需要像新车用户那样更加精细化\n",
    "fin_rule1=fin_rule1.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule1=fin_rule1.select(need_cols).dropDuplicates()\n",
    "\n",
    "fin_rule1=fin_rule1.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule1=broadcast(fin_rule1)\n",
    "\n",
    "# print(fin_rule1.count())\n",
    "# print(fin_rule1.select('vin').distinct().count())\n",
    "\n",
    "check_time(s1)\n",
    "\n",
    "fin_rule1.toPandas().to_csv('rule1.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群2:有首保,无常规,预测第1次常规"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 3.943304936091105\n"
     ]
    }
   ],
   "source": [
    "def get_rule_2(baoyangdf):\n",
    "    rule1=baoyangdf.filter(col('the_group')=='有首保,无常规,预测第1次常规')\n",
    "    if 'baoyang_id' in rule1.columns:\n",
    "        rule1=rule1.drop('baoyang_id')\n",
    "    rule1=rule1.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    rule1=get_last_run(rule1)\n",
    "    \n",
    "    rule11=rule1.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date','the_group').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile','repair_day','mile']:\n",
    "        tmp=rule1.groupBy('vin').pivot('baoyang_id',[1]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        rule11=rule11.join(tmp,on='vin',how='left')\n",
    "    rule11=rule11.withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500))\n",
    "    \n",
    "    rule11=rule11.dropDuplicates()\n",
    "\n",
    "    return rule11\n",
    "\n",
    "def get_trend_2(baoyangdf):\n",
    "    df1=baoyangdf.filter((col('shoubao_times')==1)&(col('changgui_times')>=1))\n",
    "    \n",
    "    if 'baoyang_id' in df1.columns:\n",
    "        df1=df1.drop('baoyang_id')\n",
    "        \n",
    "    df1=df1.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    df1=df1.filter(col('baoyang_id')<=2)\n",
    "    df1=get_last_run(df1)\n",
    "    \n",
    "    df11=df1.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=df1.groupBy('vin').pivot('baoyang_id',[1,2]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        df11=df11.join(tmp,on='vin',how='left')\n",
    "    df11=df11.filter((col('daydiff_1')>0)&(col('daydiff_2')>0)&(col('milediff_1')>0)&(col('milediff_2')>0))\n",
    "    \n",
    "    df11=df11.withColumn('daydiff_2_1',col('daydiff_2')/col('daydiff_1')).\\\n",
    "    withColumn('milediff_2_1',col('milediff_2')/(col('milediff_1')+1)).\\\n",
    "    withColumn('dayofmile_2_1',col('dayofmile_2')/(col('dayofmile_1')+0.01)).\\\n",
    "    withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500))\n",
    "\n",
    "    return df11\n",
    "\n",
    "def nihe_2(df00):\n",
    "    df00=df00.toPandas()\n",
    "\n",
    "    daydiff_trend_median=df00['daydiff_2_1'].median()\n",
    "#     print(daydiff_trend_median)\n",
    "    milediff_trend_median=df00['milediff_2_1'].median()\n",
    "#     print(milediff_trend_median)\n",
    "    dayofmile_trend_median=df00['dayofmile_2_1'].median()\n",
    "#     print(dayofmile_trend_median)\n",
    "    \n",
    "    daydiff_median=df00['daydiff_%d'%(end_id+1)].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_%d'%(end_id+1)].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_%d'%(end_id+1)].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['daydiff_1_bin','milediff_1_bin']\n",
    "    nihe0=df00.groupby(key0)['daydiff_2_1','milediff_2_1','dayofmile_2_1'].median()\n",
    "    nihe0.columns=['first_'+x for x in ['daydiff_2_1','milediff_2_1','dayofmile_2_1']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField(key0[0], StringType(), True),\n",
    "                        StructField(key0[1], StringType(), True),\n",
    "                        StructField(\"first_daydiff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"first_milediff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"first_dayofmile_2_1\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "    \n",
    "    key1=['daydiff_1_bin']\n",
    "    nihe1=df00.groupby(key1)['daydiff_2_1','milediff_2_1','dayofmile_2_1'].median()\n",
    "    nihe1.columns=['second_'+x for x in ['daydiff_2_1','milediff_2_1','dayofmile_2_1']]\n",
    "    nihe1=nihe1.reset_index()\n",
    "    schema1=StructType([StructField(key1[0], StringType(), True),\n",
    "                        StructField(\"second_daydiff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"second_milediff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"second_dayofmile_2_1\",DoubleType(),True)])\n",
    "    nihe1=spark.createDataFrame(nihe1,schema=schema1)\n",
    "    return key0,key1,nihe0,nihe1,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median\n",
    "\n",
    "s1=time.time()\n",
    "rule2=get_rule_2(baoyangdf)\n",
    "trend2=get_trend_2(baoyangdf)\n",
    "key0,key1,nihe0,nihe1,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median=nihe_2(trend2)\n",
    "rule2=rule2.join(nihe0,on=key0,how='left').join(nihe1,on=key1,how='left')\n",
    "\n",
    "rule2=rule2.withColumn('next_daydiff',col('first_daydiff_2_1')).\\\n",
    "withColumn('next_daydiff',when((col('next_daydiff').isNull())&(col('second_daydiff_2_1').isNotNull()),col('second_daydiff_2_1')).\\\n",
    "           when(col('next_daydiff').isNull(),lit(daydiff_trend_median)).otherwise(col('next_daydiff')))\n",
    "rule2=rule2.withColumn('next_milediff',col('first_milediff_2_1')).\\\n",
    "withColumn('next_milediff',when((col('next_milediff').isNull())&(col('second_milediff_2_1').isNotNull()),col('second_milediff_2_1')).\\\n",
    "           when(col('next_milediff').isNull(),lit(milediff_trend_median)).otherwise(col('next_milediff')))\n",
    "rule2=rule2.withColumn('next_dayofmile',col('first_dayofmile_2_1')).\\\n",
    "withColumn('next_dayofmile',when((col('next_dayofmile').isNull())&(col('second_dayofmile_2_1').isNotNull()),col('second_dayofmile_2_1')).\\\n",
    "           when(col('next_dayofmile').isNull(),lit(dayofmile_trend_median)).otherwise(col('next_dayofmile')))\n",
    "\n",
    "end_id=1\n",
    "fin_rule2=get_next_date_mile(rule2,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule2=fin_rule2.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule2=fin_rule2.select(need_cols).dropDuplicates()\n",
    "fin_rule2=fin_rule2.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule2=broadcast(fin_rule2)\n",
    "\n",
    "# print(fin_rule2.count())\n",
    "# print(fin_rule2.select('vin').distinct().count())\n",
    "fin_rule2.toPandas().to_csv('rule2.csv',index=False,encoding='utf-8')\n",
    "\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群3:无首保,1次常规,预测第2次常规(购车)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 3.976809581120809\n"
     ]
    }
   ],
   "source": [
    "def get_trend_3(baoyangdf):\n",
    "    df2=baoyangdf.filter(col('changgui_times')>=2)\n",
    "    df2=df2.filter(col('first_maintance')!=1)\n",
    "    \n",
    "    if 'baoyang_id' in df2.columns:\n",
    "        df2=df2.drop('baoyang_id')\n",
    "        \n",
    "    df2=df2.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    df2=df2.filter(col('baoyang_id')<=2)\n",
    "    df2=get_last_run(df2)\n",
    "    \n",
    "    df22=df2.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=df2.groupBy('vin').pivot('baoyang_id',[1,2]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        df22=df22.join(tmp,on='vin',how='left')\n",
    "    df22=df22.filter((col('daydiff_1')>0)&(col('daydiff_2')>0)&(col('milediff_1')>0)&(col('milediff_2')>0))\n",
    "\n",
    "    df22=df22.withColumn('daydiff_2_1',col('daydiff_2')/col('daydiff_1')).\\\n",
    "    withColumn('milediff_2_1',col('milediff_2')/(col('milediff_1')+1)).\\\n",
    "    withColumn('dayofmile_2_1',col('dayofmile_2')/(col('dayofmile_1')+0.01)).\\\n",
    "    withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500))\n",
    "\n",
    "    return df22\n",
    "\n",
    "def get_rule_3(baoyangdf):\n",
    "    rule2=baoyangdf.filter(col('the_group')=='无首保,1次常规,预测第2次常规(购车)')\n",
    "    rule2=get_last_run(rule2)\n",
    "    if 'baoyang_id' in rule2.columns:\n",
    "        rule2=rule2.drop('baoyang_id')\n",
    "    rule2=rule2.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    rule2=get_last_run(rule2)\n",
    "    \n",
    "    rule22=rule2.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date','the_group').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile','repair_day','mile']:\n",
    "        tmp=rule2.groupBy('vin').pivot('baoyang_id',[1]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        rule22=rule22.join(tmp,on='vin',how='left')\n",
    "    rule22=rule22.withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500))\n",
    "    \n",
    "    rule22=rule22.dropDuplicates()\n",
    "\n",
    "    return rule22\n",
    "\n",
    "def nihe_3(df00):\n",
    "    df00=df00.toPandas()\n",
    "    \n",
    "    daydiff_trend_median=df00['daydiff_2_1'].median()\n",
    "#     print(daydiff_trend_median)\n",
    "    milediff_trend_median=df00['milediff_2_1'].median()\n",
    "#     print(milediff_trend_median)\n",
    "    dayofmile_trend_median=df00['dayofmile_2_1'].median()\n",
    "#     print(dayofmile_trend_median)\n",
    "    \n",
    "    daydiff_median=df00['daydiff_%d'%(end_id+1)].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_%d'%(end_id+1)].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_%d'%(end_id+1)].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['daydiff_1_bin','milediff_1_bin']\n",
    "    nihe0=df00.groupby(key0)['daydiff_2_1','milediff_2_1','dayofmile_2_1'].median()\n",
    "    nihe0.columns=['first_'+x for x in ['daydiff_2_1','milediff_2_1','dayofmile_2_1']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField(key0[0], StringType(), True),\n",
    "                        StructField(key0[1], StringType(), True),\n",
    "                        StructField(\"first_daydiff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"first_milediff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"first_dayofmile_2_1\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "    \n",
    "    key1=['daydiff_1_bin']\n",
    "    nihe1=df00.groupby(key1)['daydiff_2_1','milediff_2_1','dayofmile_2_1'].median()\n",
    "    nihe1.columns=['second_'+x for x in ['daydiff_2_1','milediff_2_1','dayofmile_2_1']]\n",
    "    nihe1=nihe1.reset_index()\n",
    "    schema1=StructType([StructField(key1[0], StringType(), True),\n",
    "                        StructField(\"second_daydiff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"second_milediff_2_1\", DoubleType(), True),\n",
    "                        StructField(\"second_dayofmile_2_1\",DoubleType(),True)])\n",
    "    nihe1=spark.createDataFrame(nihe1,schema=schema1)\n",
    "    return key0,key1,nihe0,nihe1,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median\n",
    "\n",
    "s1=time.time()\n",
    "rule3=get_rule_3(baoyangdf)\n",
    "trend3=get_trend_3(baoyangdf)\n",
    "key0,key1,nihe0,nihe1,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median=nihe_3(trend3)\n",
    "rule3=rule3.join(nihe0,on=key0,how='left').join(nihe1,on=key1,how='left')\n",
    "\n",
    "rule3=rule3.withColumn('next_daydiff',col('first_daydiff_2_1')).\\\n",
    "withColumn('next_daydiff',when((col('next_daydiff').isNull())&(col('second_daydiff_2_1').isNotNull()),col('second_daydiff_2_1')).\\\n",
    "           when(col('next_daydiff').isNull(),lit(daydiff_trend_median)).otherwise(col('next_daydiff'))).\\\n",
    "withColumn('next_milediff',col('first_milediff_2_1')).\\\n",
    "withColumn('next_milediff',when((col('next_milediff').isNull())&(col('second_milediff_2_1').isNotNull()),col('second_milediff_2_1')).when((col('next_milediff').isNull())&(col('second_milediff_2_1').isNull()),lit(milediff_trend_median)).otherwise(col('next_milediff'))).\\\n",
    "withColumn('next_dayofmile',col('first_dayofmile_2_1')).\\\n",
    "withColumn('next_dayofmile',when((col('next_dayofmile').isNull())&(col('second_dayofmile_2_1').isNotNull()),col('second_dayofmile_2_1')).when((col('next_dayofmile').isNull())&(col('second_dayofmile_2_1').isNull()),lit(dayofmile_trend_median)).otherwise(col('next_dayofmile')))\n",
    "\n",
    "end_id=1\n",
    "fin_rule3=get_next_date_mile(rule3,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule3=fin_rule3.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule3=fin_rule3.select(need_cols).dropDuplicates()\n",
    "\n",
    "fin_rule3=fin_rule3.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule3=broadcast(fin_rule3)\n",
    "\n",
    "# print(fin_rule3.count())\n",
    "# print(fin_rule3.select('vin').distinct().count())\n",
    "\n",
    "fin_rule3.toPandas().to_csv('rule3.csv',index=False,encoding='utf-8')\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群4:有首保,1常规,预测第2次常规"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trend_4(baoyangdf):\n",
    "    df2=baoyangdf.filter((col('shoubao_times')==1)&(col('changgui_times')>=2))\n",
    "    if 'baoyang_id' in df2.columns:\n",
    "        df2=df2.drop('baoyang_id')\n",
    "    df2=df2.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    df2=df2.filter(col('baoyang_id')<=3)\n",
    "    df2=get_last_run(df2)\n",
    "    \n",
    "    df22=df2.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=df2.groupBy('vin').pivot('baoyang_id',[1,2,3]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        df22=df22.join(tmp,on='vin',how='left')\n",
    "    df22=df22.filter((col('daydiff_1')>0)&(col('daydiff_2')>0)&(col('daydiff_3')>0)&(col('milediff_1')>0)&(col('milediff_2')>0)&(col('milediff_3')>0))\n",
    "\n",
    "    df22=df22.withColumn('daydiff_2_1',col('daydiff_2')/col('daydiff_1')).\\\n",
    "    withColumn('milediff_2_1',col('milediff_2')/(col('milediff_1')+1)).\\\n",
    "    withColumn('dayofmile_2_1',col('dayofmile_2')/(col('dayofmile_1')+0.01)).\\\n",
    "    withColumn('daydiff_3_2',col('daydiff_3')/col('daydiff_2')).\\\n",
    "    withColumn('milediff_3_2',col('milediff_3')/col('milediff_2')).\\\n",
    "    withColumn('dayofmile_3_2',col('dayofmile_3')/col('dayofmile_2')).\\\n",
    "    withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500)).\\\n",
    "    withColumn('daydiff_2_bin',ceil(col('daydiff_2')/15)).\\\n",
    "    withColumn('milediff_2_bin',ceil(col('milediff_2')/2500))\n",
    "    \n",
    "    df22=df22.dropDuplicates()\n",
    "    \n",
    "    return df22\n",
    "\n",
    "def get_rule_4(baoyangdf):\n",
    "    rule3=baoyangdf.filter(col('the_group')=='有首保,1常规,预测第2次常规')\n",
    "    if 'baoyang_id' in rule3.columns:\n",
    "        rule3=rule3.drop('baoyang_id')\n",
    "    rule3=rule3.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    rule3=get_last_run(rule3)\n",
    "    \n",
    "    rule33=rule3.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date','the_group').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile','repair_day','mile']:\n",
    "        tmp=rule3.groupBy('vin').pivot('baoyang_id',[1,2]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        rule33=rule33.join(tmp,on='vin',how='left')\n",
    "    rule33=rule33.withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500)).\\\n",
    "    withColumn('daydiff_2_bin',ceil(col('daydiff_2')/15)).\\\n",
    "    withColumn('milediff_2_bin',ceil(col('milediff_2')/2500))\n",
    "    \n",
    "    rule33=rule33.drop_duplicates()\n",
    "    \n",
    "    return rule33\n",
    "\n",
    "def nihe_4(df00):\n",
    "    df00=df00.toPandas()\n",
    "    \n",
    "    daydiff_trend_median=df00['daydiff_3_2'].median()\n",
    "#     print(daydiff_trend_median)\n",
    "    milediff_trend_median=df00['milediff_3_2'].median()\n",
    "#     print(milediff_trend_median)\n",
    "    dayofmile_trend_median=df00['dayofmile_3_2'].median()\n",
    "#     print(dayofmile_trend_median)\n",
    "    \n",
    "    daydiff_median=df00['daydiff_%d'%(end_id+1)].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_%d'%(end_id+1)].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_%d'%(end_id+1)].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['daydiff_1_bin','milediff_1_bin','daydiff_2_bin','milediff_2_bin']\n",
    "    nihe0=df00.groupby(key0)['daydiff_3_2','milediff_3_2','dayofmile_3_2'].median()\n",
    "    nihe0.columns=['first_'+x for x in ['daydiff_3_2','milediff_3_2','dayofmile_3_2']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField(key0[0], StringType(), True),\n",
    "                        StructField(key0[1], StringType(), True),\n",
    "                        StructField(key0[2], StringType(), True),\n",
    "                        StructField(key0[3], StringType(), True),\n",
    "                        StructField(\"first_daydiff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"first_milediff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"first_dayofmile_3_2\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "    \n",
    "    \n",
    "    key1=['daydiff_1_bin','daydiff_2_bin','ies_name']\n",
    "    nihe1=df00.groupby(key1)['daydiff_3_2','milediff_3_2','dayofmile_3_2'].median()\n",
    "    nihe1.columns=['second_'+x for x in ['daydiff_3_2','milediff_3_2','dayofmile_3_2']]\n",
    "    nihe1=nihe1.reset_index()\n",
    "    schema1=StructType([StructField(key1[0], StringType(), True),\n",
    "                        StructField(key1[1], StringType(), True),\n",
    "                        StructField(key1[2], StringType(), True),\n",
    "                        StructField(\"second_daydiff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"second_milediff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"second_dayofmile_3_2\",DoubleType(),True)])\n",
    "    nihe1=spark.createDataFrame(nihe1,schema=schema1)\n",
    "    \n",
    "    key2=['daydiff_1_bin','daydiff_2_bin']\n",
    "    nihe2=df00.groupby(key2)['daydiff_3_2','milediff_3_2','dayofmile_3_2'].median()\n",
    "    nihe2.columns=['third_'+x for x in ['daydiff_3_2','milediff_3_2','dayofmile_3_2']]\n",
    "    nihe2=nihe2.reset_index()\n",
    "    schema2=StructType([StructField(key1[0], StringType(), True),\n",
    "                        StructField(key1[1], StringType(), True),\n",
    "                        StructField(\"third_daydiff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"third_milediff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"third_dayofmile_3_2\",DoubleType(),True)])\n",
    "    nihe2=spark.createDataFrame(nihe2,schema=schema2)\n",
    "\n",
    "    return key0,key1,key2,nihe0,nihe1,nihe2,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 4.428108660380046\n"
     ]
    }
   ],
   "source": [
    "s1=time.time()\n",
    "rule4=get_rule_4(baoyangdf)\n",
    "# print(rule4.select('vin').distinct().count())\n",
    "trend4=get_trend_4(baoyangdf)\n",
    "# print(trend4.select('vin').distinct().count())\n",
    "\n",
    "key0,key1,key2,nihe0,nihe1,nihe2,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median=nihe_4(trend4)\n",
    "rule4=rule4.join(nihe0,on=key0,how='left').join(nihe1,on=key1,how='left').join(nihe2,on=key2,how='left')\n",
    "\n",
    "rule4=rule4.withColumn('next_daydiff',col('first_daydiff_3_2')).\\\n",
    "withColumn('next_daydiff',when((col('next_daydiff').isNull())&(col('second_daydiff_3_2').isNotNull()),col('second_daydiff_3_2')).\\\n",
    "           when((col('next_daydiff').isNull())&(col('second_daydiff_3_2').isNull())&(col('third_daydiff_3_2').isNotNull()),col('third_daydiff_3_2')).\\\n",
    "           when((col('next_daydiff').isNull())&(col('second_daydiff_3_2').isNull())&(col('third_daydiff_3_2').isNull()),lit(daydiff_trend_median)).otherwise(col('next_daydiff'))).\\\n",
    "withColumn('next_milediff',col('first_milediff_3_2')).\\\n",
    "withColumn('next_milediff',when((col('next_milediff').isNull())&(col('second_milediff_3_2').isNotNull()),col('second_milediff_3_2')).\\\n",
    "           when((col('next_milediff').isNull())&(col('second_milediff_3_2').isNull())&(col('third_milediff_3_2').isNotNull()),col('third_milediff_3_2')).\\\n",
    "           when((col('next_milediff').isNull())&(col('second_milediff_3_2').isNull())&(col('third_milediff_3_2').isNull()),lit(milediff_trend_median)).otherwise(col('next_milediff'))).\\\n",
    "withColumn('next_dayofmile',col('first_dayofmile_3_2')).\\\n",
    "withColumn('next_dayofmile',when((col('next_dayofmile').isNull())&(col('second_dayofmile_3_2').isNotNull()),col('second_dayofmile_3_2')).\\\n",
    "           when((col('next_dayofmile').isNull())&(col('second_dayofmile_3_2').isNull())&(col('third_dayofmile_3_2').isNotNull()),col('third_dayofmile_3_2')).\\\n",
    "           when((col('next_dayofmile').isNull())&(col('second_dayofmile_3_2').isNull())&(col('third_dayofmile_3_2').isNull()),lit(dayofmile_trend_median)).otherwise(col('next_dayofmile')))\n",
    "\n",
    "end_id=2\n",
    "fin_rule4=get_next_date_mile(rule4,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule4=fin_rule4.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule4=fin_rule4.select(need_cols).dropDuplicates()\n",
    "fin_rule4=fin_rule4.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule4=broadcast(fin_rule4)\n",
    "\n",
    "# print(fin_rule4.count())\n",
    "# print(fin_rule4.select('vin').distinct().count())\n",
    "\n",
    "fin_rule4.toPandas().to_csv('rule4.csv',index=False,encoding='utf-8')\n",
    "\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群5:无首保,2次常规,预测第3次常规"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 4.3644527475039165\n"
     ]
    }
   ],
   "source": [
    "def get_trend_5(baoyangdf):\n",
    "    df2=baoyangdf.filter(col('changgui_times')>=3)\n",
    "    df2=df2.filter(col('first_maintance')!=1)\n",
    "    \n",
    "    if 'baoyang_id' in df2.columns:\n",
    "        df2=df2.drop('baoyang_id')\n",
    "    df2=df2.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    df2=df2.filter(col('baoyang_id')<=3)\n",
    "    df2=get_last_run(df2)\n",
    "    \n",
    "    df22=df2.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date').dropDuplicates()\n",
    "    for x in ['daydiff','milediff','dayofmile']:\n",
    "        tmp=df2.groupBy('vin').pivot('baoyang_id',[1,2,3]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        df22=df22.join(tmp,on='vin',how='left')\n",
    "    df22=df22.filter((col('daydiff_1')>0)&(col('daydiff_2')>0)&(col('daydiff_3')>0)&(col('milediff_1')>0)&(col('milediff_2')>0)&(col('milediff_3')>0))\n",
    "\n",
    "    df22=df22.withColumn('daydiff_2_1',col('daydiff_2')/col('daydiff_1')).\\\n",
    "    withColumn('milediff_2_1',col('milediff_2')/(col('milediff_1')+1)).\\\n",
    "    withColumn('dayofmile_2_1',col('dayofmile_2')/(col('dayofmile_1')+0.01)).\\\n",
    "    withColumn('daydiff_3_2',col('daydiff_3')/col('daydiff_2')).\\\n",
    "    withColumn('milediff_3_2',col('milediff_3')/col('milediff_2')).\\\n",
    "    withColumn('dayofmile_3_2',col('dayofmile_3')/col('dayofmile_2')).\\\n",
    "    withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500)).\\\n",
    "    withColumn('daydiff_2_bin',ceil(col('daydiff_2')/15)).\\\n",
    "    withColumn('milediff_2_bin',ceil(col('milediff_2')/2500))\n",
    "    \n",
    "    df22=df22.dropDuplicates()\n",
    "\n",
    "    return df22\n",
    "\n",
    "def get_rule_5(baoyangdf):\n",
    "    rule4=baoyangdf.filter(col('the_group')=='无首保,2次常规,预测第3次常规')\n",
    "    if 'baoyang_id' in rule4.columns:\n",
    "        rule4=rule4.drop('baoyang_id')\n",
    "    rule4=rule4.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(asc('repair_date'))))\n",
    "    rule4=get_last_run(rule4)\n",
    "    \n",
    "    rule44=rule4.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date','the_group').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile','repair_day','mile']:\n",
    "        tmp=rule4.groupBy('vin').pivot('baoyang_id',[1,2]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        rule44=rule44.join(tmp,on='vin',how='left')\n",
    "    rule44=rule44.withColumn('daydiff_1_bin',ceil(col('daydiff_1')/15)).\\\n",
    "    withColumn('milediff_1_bin',ceil(col('milediff_1')/2500)).\\\n",
    "    withColumn('daydiff_2_bin',ceil(col('daydiff_2')/15)).\\\n",
    "    withColumn('milediff_2_bin',ceil(col('milediff_2')/2500))\n",
    "    \n",
    "    rule44=rule44.drop_duplicates()\n",
    "    return rule44\n",
    "\n",
    "def nihe_5(df00):\n",
    "    df00=df00.toPandas()\n",
    "    \n",
    "    daydiff_trend_median=df00['daydiff_3_2'].median()\n",
    "#     print(daydiff_trend_median)\n",
    "    milediff_trend_median=df00['milediff_3_2'].median()\n",
    "#     print(milediff_trend_median)\n",
    "    dayofmile_trend_median=df00['dayofmile_3_2'].median()\n",
    "#     print(dayofmile_trend_median)\n",
    "    \n",
    "    daydiff_median=df00['daydiff_%d'%(end_id+1)].median()\n",
    "#     print(daydiff_median)\n",
    "    milediff_median=df00['milediff_%d'%(end_id+1)].median()\n",
    "#     print(milediff_median)\n",
    "    dayofmile_median=df00['dayofmile_%d'%(end_id+1)].median()\n",
    "#     print(dayofmile_median)\n",
    "    \n",
    "    key0=['daydiff_1_bin','milediff_1_bin','daydiff_2_bin','milediff_2_bin']\n",
    "    nihe0=df00.groupby(key0)['daydiff_3_2','milediff_3_2','dayofmile_3_2'].median()\n",
    "    nihe0.columns=['first_'+x for x in ['daydiff_3_2','milediff_3_2','dayofmile_3_2']]\n",
    "    nihe0=nihe0.reset_index()\n",
    "    schema0=StructType([StructField(key0[0], StringType(), True),\n",
    "                        StructField(key0[1], StringType(), True),\n",
    "                        StructField(key0[2], StringType(), True),\n",
    "                        StructField(key0[3], StringType(), True),\n",
    "                        StructField(\"first_daydiff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"first_milediff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"first_dayofmile_3_2\",DoubleType(),True)])\n",
    "    nihe0=spark.createDataFrame(nihe0,schema=schema0)\n",
    "    \n",
    "    \n",
    "    key1=['daydiff_1_bin','daydiff_2_bin','ies_name']\n",
    "    nihe1=df00.groupby(key1)['daydiff_3_2','milediff_3_2','dayofmile_3_2'].median()\n",
    "    nihe1.columns=['second_'+x for x in ['daydiff_3_2','milediff_3_2','dayofmile_3_2']]\n",
    "    nihe1=nihe1.reset_index()\n",
    "    schema1=StructType([StructField(key1[0], StringType(), True),\n",
    "                        StructField(key1[1], StringType(), True),\n",
    "                        StructField(key1[2], StringType(), True),\n",
    "                        StructField(\"second_daydiff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"second_milediff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"second_dayofmile_3_2\",DoubleType(),True)])\n",
    "    nihe1=spark.createDataFrame(nihe1,schema=schema1)\n",
    "    \n",
    "    key2=['daydiff_1_bin','daydiff_2_bin']\n",
    "    nihe2=df00.groupby(key2)['daydiff_3_2','milediff_3_2','dayofmile_3_2'].median()\n",
    "    nihe2.columns=['third_'+x for x in ['daydiff_3_2','milediff_3_2','dayofmile_3_2']]\n",
    "    nihe2=nihe2.reset_index()\n",
    "    schema2=StructType([StructField(key1[0], StringType(), True),\n",
    "                        StructField(key1[1], StringType(), True),\n",
    "                        StructField(\"third_daydiff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"third_milediff_3_2\", DoubleType(), True),\n",
    "                        StructField(\"third_dayofmile_3_2\",DoubleType(),True)])\n",
    "    nihe2=spark.createDataFrame(nihe2,schema=schema2)\n",
    "\n",
    "    return key0,key1,key2,nihe0,nihe1,nihe2,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median\n",
    "\n",
    "s1=time.time()\n",
    "rule5=get_rule_5(baoyangdf)\n",
    "# print(rule5.select('vin').distinct().count())\n",
    "trend5=get_trend_5(baoyangdf)\n",
    "# print(trend5.select('vin').distinct().count())\n",
    "\n",
    "key0,key1,key2,nihe0,nihe1,nihe2,daydiff_trend_median,milediff_trend_median,dayofmile_trend_median,daydiff_median,milediff_median,dayofmile_median=nihe_5(trend5)\n",
    "rule5=rule5.join(nihe0,on=key0,how='left').join(nihe1,on=key1,how='left').join(nihe2,on=key2,how='left')\n",
    "\n",
    "rule5=rule5.withColumn('next_daydiff',col('first_daydiff_3_2')).\\\n",
    "withColumn('next_daydiff',when((col('next_daydiff').isNull())&(col('second_daydiff_3_2').isNotNull()),col('second_daydiff_3_2')).when((col('next_daydiff').isNull())&(col('second_daydiff_3_2').isNull())&(col('third_daydiff_3_2').isNotNull()),col('third_daydiff_3_2')).when((col('next_daydiff').isNull())&(col('second_daydiff_3_2').isNull())&(col('third_daydiff_3_2').isNull()),lit(daydiff_trend_median)).otherwise(col('next_daydiff'))).\\\n",
    "withColumn('next_milediff',col('first_milediff_3_2')).\\\n",
    "withColumn('next_milediff',when((col('next_milediff').isNull())&(col('second_milediff_3_2').isNotNull()),col('second_milediff_3_2')).when((col('next_milediff').isNull())&(col('second_milediff_3_2').isNull())&(col('third_milediff_3_2').isNotNull()),col('third_milediff_3_2')).when((col('next_milediff').isNull())&(col('second_milediff_3_2').isNull())&(col('third_milediff_3_2').isNull()),lit(milediff_trend_median)).otherwise(col('next_milediff'))).\\\n",
    "withColumn('next_dayofmile',col('first_dayofmile_3_2')).\\\n",
    "withColumn('next_dayofmile',when((col('next_dayofmile').isNull())&(col('second_dayofmile_3_2').isNotNull()),col('second_dayofmile_3_2')).when((col('next_dayofmile').isNull())&(col('second_dayofmile_3_2').isNull())&(col('third_dayofmile_3_2').isNotNull()),col('third_dayofmile_3_2')).when((col('next_dayofmile').isNull())&(col('second_dayofmile_3_2').isNull())&(col('third_dayofmile_3_2').isNull()),lit(dayofmile_trend_median)).otherwise(col('next_dayofmile')))\n",
    "\n",
    "end_id=2\n",
    "fin_rule5=get_next_date_mile(rule5,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule5=fin_rule5.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule5=fin_rule5.select(need_cols).dropDuplicates()\n",
    "fin_rule5=fin_rule5.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule5=broadcast(fin_rule5)\n",
    "\n",
    "# print(fin_rule5.count())\n",
    "# print(fin_rule5.select('vin').distinct().count())\n",
    "\n",
    "fin_rule5.toPandas().to_csv('rule5.csv',index=False,encoding='utf-8')\n",
    "\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群6:有首保,2次常规,预测第3次常规"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 3.5460384686787925\n"
     ]
    }
   ],
   "source": [
    "def get_rule_6(baoyangdf):\n",
    "    rule4=baoyangdf.filter(col('the_group')=='有首保,2次常规,预测第3次常规')\n",
    "    rule4=get_last_run(rule4) \n",
    "    \n",
    "#     print(\"注意,此处的baoyang_id是倒序!!!\")\n",
    "    if 'baoyang_id' in rule4.columns:\n",
    "        rule4=rule4.drop('baoyang_id')\n",
    "    rule4=rule4.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    rule4=rule4.filter(col('baoyang_id')<=2)\n",
    "    \n",
    "    daydiff_median=rule4.approxQuantile('daydiff',[0.5],0.01)[0]\n",
    "    milediff_median=rule4.approxQuantile('milediff',[0.5],0.01)[0]\n",
    "    dayofmile_median=rule4.approxQuantile('dayofmile',[0.5],0.01)[0]\n",
    "#     print(daydiff_median,milediff_median,dayofmile_median)\n",
    "    \n",
    "#     print('由于规则中还有天数差的脏数据存在,所以进行平滑一下')\n",
    "    milediff_right,milediff_left=rule4.approxQuantile('milediff',[0.02,0.98],0.01)\n",
    "    \n",
    "    rule4=rule4.withColumn('milediff',when(col('milediff')<milediff_right,lit(milediff_right)).when(col('milediff')>milediff_left,lit(milediff_left)).otherwise(col('milediff')))\n",
    "    if 'dayofmile' in rule4.columns: #会有负数,所以需要重置\n",
    "        rule4=rule4.drop('dayofmile')\n",
    "        rule4=rule4.withColumn('dayofmile',col('milediff')/col('daydiff'))\n",
    "    \n",
    "    rule44=rule4.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date','the_group').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile','repair_day','mile']:\n",
    "        tmp=rule4.groupBy('vin').pivot('baoyang_id',[1,2]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        rule44=rule44.join(tmp,on='vin',how='left')\n",
    "    \n",
    "#     print('开始计算vin最近二次日均行驶里程的趋势')\n",
    "    rule44=rule44.withColumn('milediff_mean',(col('milediff_1')+col('milediff_2'))/2).\\\n",
    "    withColumn('milediff_sub',abs(col('milediff_1')-col('milediff_2'))).\\\n",
    "    withColumn('milediff_per',col('milediff_sub')/col('milediff_mean')).\\\n",
    "    withColumn('daydiff_mean',(col('daydiff_1')+col('daydiff_2'))/2).\\\n",
    "    withColumn('daydiff_sub',abs(col('milediff_1')-col('milediff_2'))).\\\n",
    "    withColumn('daydiff_per',col('daydiff_sub')/col('daydiff_mean')).\\\n",
    "    withColumn('dayofmile_mean',(col('dayofmile_1')+col('dayofmile_2'))/2).\\\n",
    "    withColumn('dayofmile_sub',abs(col('dayofmile_1')-col('dayofmile_2'))).\\\n",
    "    withColumn('dayofmile_per',col('dayofmile_sub')/col('dayofmile_mean'))\n",
    "    \n",
    "#     print('注意这里的1和2,与上面的这些批次都是不一样的,是倒序,是倒序,是倒序!')\n",
    "    rule44=rule44.withColumn('milediff_pred',when(col('milediff_per')<=0.1,col('milediff_1')*0.55+col('milediff_2')*0.45).\\\n",
    "                             when((col('milediff_per')>0.1)&(col('milediff_per')<=0.2),col('milediff_1')*0.6+col('milediff_2')*0.4).\\\n",
    "                             when((col('milediff_per')>0.2)&(col('milediff_per')<=0.3),col('milediff_1')*0.65+col('milediff_2')*0.35).\\\n",
    "                             when((col('milediff_per')>0.3)&(col('milediff_per')<=0.4),col('milediff_1')*0.7+col('milediff_2')*0.3).\\\n",
    "                             when((col('milediff_per')>0.4)&(col('milediff_per')<=0.5),col('milediff_1')*0.75+col('milediff_2')*0.25).\\\n",
    "                             when((col('milediff_per')>0.5)&(col('milediff_per')<=0.6),col('milediff_1')*0.8+col('milediff_2')*0.2).\\\n",
    "                             when((col('milediff_per')>0.6)&(col('milediff_per')<=0.7),col('milediff_1')*0.85+col('milediff_2')*0.15).\\\n",
    "                             when((col('milediff_per')>0.7)&(col('milediff_per')<=0.8),col('milediff_1')*0.9+col('milediff_2')*0.1).\\\n",
    "                             when((col('milediff_per')>0.8)&(col('milediff_per')<=0.9),col('milediff_1')*0.95+col('milediff_2')*0.05).\\\n",
    "                             when((col('milediff_per')>0.9),col('milediff_1')*1))\n",
    "    \n",
    "    rule44=rule44.withColumn('daydiff_pred',when(col('daydiff_per')<=0.1,col('daydiff_1')*0.55+col('daydiff_2')*0.45).\\\n",
    "                             when((col('daydiff_per')>0.1)&(col('daydiff_per')<=0.2),col('daydiff_1')*0.6+col('daydiff_2')*0.4).\\\n",
    "                             when((col('daydiff_per')>0.2)&(col('daydiff_per')<=0.3),col('daydiff_1')*0.65+col('daydiff_2')*0.35).\\\n",
    "                             when((col('daydiff_per')>0.3)&(col('daydiff_per')<=0.4),col('daydiff_1')*0.7+col('daydiff_2')*0.3).\\\n",
    "                             when((col('daydiff_per')>0.4)&(col('daydiff_per')<=0.5),col('daydiff_1')*0.75+col('daydiff_2')*0.25).\\\n",
    "                             when((col('daydiff_per')>0.5)&(col('daydiff_per')<=0.6),col('daydiff_1')*0.8+col('daydiff_2')*0.2).\\\n",
    "                             when((col('daydiff_per')>0.6)&(col('daydiff_per')<=0.7),col('daydiff_1')*0.85+col('daydiff_2')*0.15).\\\n",
    "                             when((col('daydiff_per')>0.7)&(col('daydiff_per')<=0.8),col('daydiff_1')*0.9+col('daydiff_2')*0.1).\\\n",
    "                             when((col('daydiff_per')>0.8)&(col('daydiff_per')<=0.9),col('daydiff_1')*0.95+col('daydiff_2')*0.05).\\\n",
    "                             when((col('daydiff_per')>0.9),col('daydiff_1')*1))\n",
    "    \n",
    "    rule44=rule44.withColumn('dayofmile_pred',when(col('dayofmile_per')<=0.1,col('dayofmile_1')*0.55+col('dayofmile_2')*0.45).\\\n",
    "                             when((col('dayofmile_per')>0.1)&(col('dayofmile_per')<=0.2),col('dayofmile_1')*0.6+col('dayofmile_2')*0.4).\\\n",
    "                             when((col('dayofmile_per')>0.2)&(col('dayofmile_per')<=0.3),col('dayofmile_1')*0.65+col('dayofmile_2')*0.35).\\\n",
    "                             when((col('dayofmile_per')>0.3)&(col('dayofmile_per')<=0.4),col('dayofmile_1')*0.7+col('dayofmile_2')*0.3).\\\n",
    "                             when((col('dayofmile_per')>0.4)&(col('dayofmile_per')<=0.5),col('dayofmile_1')*0.75+col('dayofmile_2')*0.25).\\\n",
    "                             when((col('dayofmile_per')>0.5)&(col('dayofmile_per')<=0.6),col('dayofmile_1')*0.8+col('dayofmile_2')*0.2).\\\n",
    "                             when((col('dayofmile_per')>0.6)&(col('dayofmile_per')<=0.7),col('dayofmile_1')*0.85+col('dayofmile_2')*0.15).\\\n",
    "                             when((col('dayofmile_per')>0.7)&(col('dayofmile_per')<=0.8),col('dayofmile_1')*0.9+col('dayofmile_2')*0.1).\\\n",
    "                             when((col('dayofmile_per')>0.8)&(col('dayofmile_per')<=0.9),col('dayofmile_1')*0.95+col('dayofmile_2')*0.05).\\\n",
    "                             when((col('dayofmile_per')>0.9),col('dayofmile_1')*1))\n",
    "    \n",
    "    rule44=rule44.withColumn('daydiff_pred',ceil('daydiff_pred')).\\\n",
    "    withColumn('milediff_pred',ceil('milediff_pred')).\\\n",
    "    withColumn('dayofmile_pred',ceil('dayofmile_pred'))\n",
    "    \n",
    "    return rule44,daydiff_median,milediff_median,dayofmile_median\n",
    "\n",
    "def get_next_date_mile_67(rule6,end_id,daydiff_median,milediff_median,dayofmile_median):\n",
    "#     print('检查脏数据情况')\n",
    "#     print('预测出来的公里数差<=0',rule6.filter(col('milediff_pred')<=0).select('vin').distinct().count())\n",
    "\n",
    "    rule6=rule6.withColumn('milediff_pred',when(col('milediff_pred')<=0,lit(milediff_median)).otherwise(col('milediff_pred'))).\\\n",
    "    withColumn('dayofmile_pred',when(col('dayofmile_pred')<=0,lit(dayofmile_median)).otherwise(col('dayofmile_pred')))\n",
    "\n",
    "    rule6=rule6.withColumn('next_mile',col('mile_%d'%end_id)+col('milediff_pred')).\\\n",
    "    withColumn('next_mile',ceil('next_mile'))\n",
    "    \n",
    "    if 'daydiff_pred' in rule6.columns:\n",
    "        rule6=rule6.drop('daydiff_pred')\n",
    "\n",
    "    rule6=rule6.withColumn('daydiff_pred',col('milediff_pred')/(col('dayofmile_pred')+0.01)).\\\n",
    "    withColumn('daydiff_pred',ceil('daydiff_pred'))\n",
    "\n",
    "#     print('检查脏数据')\n",
    "#     print('预测出来的天数差<=0',rule6.filter(col('daydiff_pred')<=0).select('vin').distinct().count())\n",
    "\n",
    "    rule6=rule6.withColumn('next_date',get_dates_udf3(col('purchase_date'),col('repair_day_1'),col('daydiff_pred')))\n",
    "\n",
    "    rule6=rule6.withColumn('next_mile',col('mile_1')+col('milediff_pred'))\n",
    "    rule6=rule6.withColumn('next_mile',ceil('next_mile'))\n",
    "\n",
    "    rule6=rule6.withColumn('last_baoyang_date',get_dates_udf2(col('purchase_date'),col('repair_day_1')))\n",
    "\n",
    "    rule6=rule6.withColumn('last_baoyang_mile',col('mile_1'))\n",
    "    \n",
    "    return rule6\n",
    "\n",
    "s1=time.time()\n",
    "rule6,daydiff_median,milediff_median,dayofmile_median=get_rule_6(baoyangdf)\n",
    "\n",
    "end_id=1\n",
    "fin_rule6=get_next_date_mile_67(rule6,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule6=fin_rule6.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule6=fin_rule6.select(need_cols).dropDuplicates()\n",
    "fin_rule6=fin_rule6.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule6=broadcast(fin_rule6)\n",
    "\n",
    "# print(fin_rule6.count())\n",
    "# print(fin_rule6.select('vin').distinct().count())\n",
    "\n",
    "fin_rule6.toPandas().to_csv('rule6.csv',index=False,encoding='utf-8')\n",
    "\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=red>规则人群7:3次及以上常规,预测接下去常规"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_7(baoyangdf):\n",
    "    rule4=baoyangdf.filter(col('the_group')=='3次及以上常规,预测接下去常规')\n",
    "    rule4=get_last_run(rule4)\n",
    "    \n",
    "    if 'baoyang_id' in rule4.columns:\n",
    "        rule4=rule4.drop('baoyang_id')\n",
    "    \n",
    "#     print(\"注意,此处的baoyang_id是倒序!!!\")\n",
    "    rule4=rule4.withColumn('baoyang_id',row_number().over(Window.partitionBy('vin').orderBy(desc('repair_date'))))\n",
    "    rule4=rule4.filter(col('baoyang_id')<=2)\n",
    "    \n",
    "    daydiff_median=rule4.approxQuantile('daydiff',[0.5],0.01)[0]\n",
    "    milediff_median=rule4.approxQuantile('milediff',[0.5],0.01)[0]\n",
    "    dayofmile_median=rule4.approxQuantile('dayofmile',[0.5],0.01)[0]\n",
    "#     print(daydiff_median,milediff_median,dayofmile_median)\n",
    "    \n",
    "#     print('由于规则中还有天数差的脏数据存在,所以进行平滑一下')\n",
    "    milediff_right,milediff_left=rule4.approxQuantile('milediff',[0.02,0.98],0.01)\n",
    "    \n",
    "    rule4=rule4.withColumn('milediff',when(col('milediff')<milediff_right,lit(milediff_right)).when(col('milediff')>milediff_left,lit(milediff_left)).otherwise(col('milediff')))\n",
    "    if 'dayofmile' in rule4.columns: #会有负数,所以需要重置\n",
    "        rule4=rule4.withColumn('dayofmile',col('milediff')/col('daydiff'))\n",
    "    \n",
    "    rule44=rule4.select('vin','belong_dealer_code','ies_name','user_tag','purchase_date','the_group').dropDuplicates()\n",
    "    \n",
    "    for x in ['daydiff','milediff','dayofmile','repair_day','mile']:\n",
    "        tmp=rule4.groupBy('vin').pivot('baoyang_id',[1,2]).agg(mean(x))\n",
    "        cols=tmp.columns\n",
    "        cols.remove('vin')\n",
    "        for y in cols:\n",
    "            tmp=tmp.withColumnRenamed(y,'%s_%s'%(x,y))\n",
    "        rule44=rule44.join(tmp,on='vin',how='left')\n",
    "    \n",
    "#     print('开始计算vin最近二次日均行驶里程的趋势')\n",
    "    rule44=rule44.withColumn('milediff_mean',(col('milediff_1')+col('milediff_2'))/2).\\\n",
    "    withColumn('milediff_sub',abs(col('milediff_1')-col('milediff_2'))).\\\n",
    "    withColumn('milediff_per',col('milediff_sub')/col('milediff_mean')).\\\n",
    "    withColumn('daydiff_mean',(col('daydiff_1')+col('daydiff_2'))/2).\\\n",
    "    withColumn('daydiff_sub',abs(col('milediff_1')-col('milediff_2'))).\\\n",
    "    withColumn('daydiff_per',col('daydiff_sub')/col('daydiff_mean')).\\\n",
    "    withColumn('dayofmile_mean',(col('dayofmile_1')+col('dayofmile_2'))/2).\\\n",
    "    withColumn('dayofmile_sub',abs(col('dayofmile_1')-col('dayofmile_2'))).\\\n",
    "    withColumn('dayofmile_per',col('dayofmile_sub')/col('dayofmile_mean'))\n",
    "    \n",
    "#     print('注意这里的1和2,与上面的这些批次都是不一样的,是倒序,是倒序,是倒序!')\n",
    "    rule44=rule44.withColumn('milediff_pred',when(col('milediff_per')<=0.1,col('milediff_1')*0.55+col('milediff_2')*0.45).\\\n",
    "                             when((col('milediff_per')>0.1)&(col('milediff_per')<=0.2),col('milediff_1')*0.6+col('milediff_2')*0.4).\\\n",
    "                             when((col('milediff_per')>0.2)&(col('milediff_per')<=0.3),col('milediff_1')*0.65+col('milediff_2')*0.35).\\\n",
    "                             when((col('milediff_per')>0.3)&(col('milediff_per')<=0.4),col('milediff_1')*0.7+col('milediff_2')*0.3).\\\n",
    "                             when((col('milediff_per')>0.4)&(col('milediff_per')<=0.5),col('milediff_1')*0.75+col('milediff_2')*0.25).\\\n",
    "                             when((col('milediff_per')>0.5)&(col('milediff_per')<=0.6),col('milediff_1')*0.8+col('milediff_2')*0.2).\\\n",
    "                             when((col('milediff_per')>0.6)&(col('milediff_per')<=0.7),col('milediff_1')*0.85+col('milediff_2')*0.15).\\\n",
    "                             when((col('milediff_per')>0.7)&(col('milediff_per')<=0.8),col('milediff_1')*0.9+col('milediff_2')*0.1).\\\n",
    "                             when((col('milediff_per')>0.8)&(col('milediff_per')<=0.9),col('milediff_1')*0.95+col('milediff_2')*0.05).\\\n",
    "                             when((col('milediff_per')>0.9),col('milediff_1')*1))\n",
    "    \n",
    "    rule44=rule44.withColumn('daydiff_pred',when(col('daydiff_per')<=0.1,col('daydiff_1')*0.55+col('daydiff_2')*0.45).\\\n",
    "                             when((col('daydiff_per')>0.1)&(col('daydiff_per')<=0.2),col('daydiff_1')*0.6+col('daydiff_2')*0.4).\\\n",
    "                             when((col('daydiff_per')>0.2)&(col('daydiff_per')<=0.3),col('daydiff_1')*0.65+col('daydiff_2')*0.35).\\\n",
    "                             when((col('daydiff_per')>0.3)&(col('daydiff_per')<=0.4),col('daydiff_1')*0.7+col('daydiff_2')*0.3).\\\n",
    "                             when((col('daydiff_per')>0.4)&(col('daydiff_per')<=0.5),col('daydiff_1')*0.75+col('daydiff_2')*0.25).\\\n",
    "                             when((col('daydiff_per')>0.5)&(col('daydiff_per')<=0.6),col('daydiff_1')*0.8+col('daydiff_2')*0.2).\\\n",
    "                             when((col('daydiff_per')>0.6)&(col('daydiff_per')<=0.7),col('daydiff_1')*0.85+col('daydiff_2')*0.15).\\\n",
    "                             when((col('daydiff_per')>0.7)&(col('daydiff_per')<=0.8),col('daydiff_1')*0.9+col('daydiff_2')*0.1).\\\n",
    "                             when((col('daydiff_per')>0.8)&(col('daydiff_per')<=0.9),col('daydiff_1')*0.95+col('daydiff_2')*0.05).\\\n",
    "                             when((col('daydiff_per')>0.9),col('daydiff_1')*1))\n",
    "    \n",
    "    rule44=rule44.withColumn('dayofmile_pred',when(col('dayofmile_per')<=0.1,col('dayofmile_1')*0.55+col('dayofmile_2')*0.45).\\\n",
    "                             when((col('dayofmile_per')>0.1)&(col('dayofmile_per')<=0.2),col('dayofmile_1')*0.6+col('dayofmile_2')*0.4).\\\n",
    "                             when((col('dayofmile_per')>0.2)&(col('dayofmile_per')<=0.3),col('dayofmile_1')*0.65+col('dayofmile_2')*0.35).\\\n",
    "                             when((col('dayofmile_per')>0.3)&(col('dayofmile_per')<=0.4),col('dayofmile_1')*0.7+col('dayofmile_2')*0.3).\\\n",
    "                             when((col('dayofmile_per')>0.4)&(col('dayofmile_per')<=0.5),col('dayofmile_1')*0.75+col('dayofmile_2')*0.25).\\\n",
    "                             when((col('dayofmile_per')>0.5)&(col('dayofmile_per')<=0.6),col('dayofmile_1')*0.8+col('dayofmile_2')*0.2).\\\n",
    "                             when((col('dayofmile_per')>0.6)&(col('dayofmile_per')<=0.7),col('dayofmile_1')*0.85+col('dayofmile_2')*0.15).\\\n",
    "                             when((col('dayofmile_per')>0.7)&(col('dayofmile_per')<=0.8),col('dayofmile_1')*0.9+col('dayofmile_2')*0.1).\\\n",
    "                             when((col('dayofmile_per')>0.8)&(col('dayofmile_per')<=0.9),col('dayofmile_1')*0.95+col('dayofmile_2')*0.05).\\\n",
    "                             when((col('dayofmile_per')>0.9),col('dayofmile_1')*1))\n",
    "    \n",
    "    rule44=rule44.withColumn('daydiff_pred',ceil('daydiff_pred')).\\\n",
    "    withColumn('milediff_pred',ceil('milediff_pred'))\n",
    "    \n",
    "    return rule44,daydiff_median,milediff_median,dayofmile_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time is: 3.5723326802253723\n"
     ]
    }
   ],
   "source": [
    "s1=time.time()\n",
    "rule7,daydiff_median,milediff_median,dayofmile_median=get_rule_7(baoyangdf)\n",
    "\n",
    "end_id=1\n",
    "fin_rule7=get_next_date_mile_67(rule7,end_id,daydiff_median,milediff_median,dayofmile_median)\n",
    "\n",
    "fin_rule7=fin_rule7.withColumn('iterated_daydiff',col('daydiff_pred')).\\\n",
    "withColumn('iterated_dayofmile',col('dayofmile_pred')).\\\n",
    "withColumn('iterated_milediff',col('milediff_pred')).\\\n",
    "withColumn('iterated_start_date',col('last_baoyang_date')).\\\n",
    "withColumn('iterated_start_mile',col('last_baoyang_mile'))\n",
    "\n",
    "fin_rule7=fin_rule7.select(need_cols).dropDuplicates()\n",
    "fin_rule7=fin_rule7.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "fin_rule7=broadcast(fin_rule7)\n",
    "\n",
    "# print(fin_rule7.count())\n",
    "# print(fin_rule7.select('vin').distinct().count())\n",
    "\n",
    "fin_rule7.toPandas().to_csv('rule7.csv',index=False,encoding='utf-8')\n",
    "\n",
    "check_time(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.09823716878891\n"
     ]
    }
   ],
   "source": [
    "print((time.time()-ft)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>合并规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule=fin_rule0.unionAll(fin_rule1).unionAll(fin_rule2).unionAll(fin_rule3).unionAll(fin_rule4).unionAll(fin_rule5).unionAll(fin_rule6).unionAll(fin_rule7)\n",
    "rule=rule.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rule=broadcast(rule)\n",
    "# print(rule.count())\n",
    "# print(rule.select('vin').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得到模型人群\n"
     ]
    }
   ],
   "source": [
    "print('得到模型人群')\n",
    "# model=spark.sql('select * from clms.next_model_result')\n",
    "model_1=spark.sql('select * from clms.fin_result_2_5')\n",
    "model_2=spark.sql('select * from clms.fin_result_9')\n",
    "model=model_1.unionAll(model_2)\n",
    "model=model.filter(col('fin_mileofday')>0)\n",
    "model=model.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "model=broadcast(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vin', 'last_baoyang_date', 'last_baoyang_mile', 'purchase_date', 'iterated_daydiff', 'iterated_milediff', 'iterated_dayofmile', 'next_baoyang_date_fin', 'next_baoyang_mile_fin', 'method']\n"
     ]
    }
   ],
   "source": [
    "model_vin=model.select('vin').distinct().withColumn('is_in_model',lit(1))\n",
    "\n",
    "rule=rule.join(model_vin,on='vin',how='left')\n",
    "rule=rule.withColumn('is_in_model',when(col('is_in_model').isNull(),lit(0)).otherwise(col('is_in_model')))\n",
    "\n",
    "the_rule=rule.filter(col('is_in_model')==0)\n",
    "the_rule=the_rule.drop('is_in_model')\n",
    "the_rule=the_rule.withColumn('next_baoyang_date_fin',col('next_date')).\\\n",
    "withColumn('next_baoyang_mile_fin',col('next_mile')).\\\n",
    "withColumn('method',lit('rule'))\n",
    "\n",
    "#模型\n",
    "model=model.withColumnRenamed('last_repair_date','last_baoyang_date').\\\n",
    "withColumnRenamed('last_repair_mile','last_baoyang_mile').\\\n",
    "withColumn('next_baoyang_date_fin',col('next_repair_date')).\\\n",
    "withColumn('next_baoyang_mile_fin',col('next_repair_mile')).\\\n",
    "withColumn('iterated_daydiff',col('daydiff_diff')).\\\n",
    "withColumn('iterated_dayofmile',col('fin_mileofday')).\\\n",
    "withColumn('iterated_milediff',col('milediff_diff')).\\\n",
    "withColumn('method',lit('model'))\n",
    "\n",
    "same_cols=[x for x in the_rule.columns if x in model.columns]\n",
    "print(same_cols)\n",
    "\n",
    "#合并\n",
    "all_df=the_rule.select(same_cols).unionAll(model.select(same_cols))\n",
    "\n",
    "if 'iterated_daydiff' in all_df.columns:\n",
    "    all_df=all_df.drop('iterated_daydiff')\n",
    "\n",
    "all_df=all_df.withColumn('iterated_daydiff',col('iterated_milediff')/col('iterated_dayofmile'))\n",
    "\n",
    "all_df=all_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "all_df=broadcast(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>开始迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_dates(df):\n",
    "    if 'today' in df.columns:\n",
    "        df=df.drop('today')\n",
    "    df=df.withColumn('today',to_date(lit(today)))\n",
    "    \n",
    "    #拆分成二个部分处理\n",
    "    df1=df.filter(col('next_baoyang_date_fin')>col('today')).\\\n",
    "    withColumn('next_baoyang_date_fin_after_today',col('next_baoyang_date_fin')).\\\n",
    "    withColumn('next_baoyang_mile_fin_after_today',col('next_baoyang_mile_fin')).\\\n",
    "    withColumn('iterated_id',lit(0)).\\\n",
    "    withColumn('rounds',lit(0))\n",
    "    \n",
    "    df2=df.filter(col('next_baoyang_date_fin')<=col('today')).\\\n",
    "    withColumn('rounds',ceil(datediff(col('today'),col('next_baoyang_date_fin'))/col('iterated_daydiff'))).\\\n",
    "    withColumn('next_baoyang_date_fin_after_today',get_dates_udf3(col('next_baoyang_date_fin'),col('iterated_daydiff'),col('rounds'))).\\\n",
    "    withColumn('next_baoyang_mile_fin_after_today',col('next_baoyang_mile_fin')+col('iterated_milediff')*col('rounds')).\\\n",
    "    withColumn('next_baoyang_mile_fin_after_today',ceil('next_baoyang_mile_fin_after_today')).withColumn('iterated_id',col('rounds')).\\\n",
    "    withColumn('rounds',col('rounds').cast('Double')).\\\n",
    "    withColumn('next_baoyang_mile_fin_after_today',col('next_baoyang_mile_fin_after_today').cast('Double')).\\\n",
    "    withColumn('iterated_id',col('iterated_id').cast('Double'))\n",
    "    \n",
    "    same_cols=[x for x in df1.columns if x in df2.columns]\n",
    "    df=df1.select(same_cols).unionAll(df2.select(same_cols))\n",
    "    \n",
    "    tongyi_cols=['vin', 'purchase_date', 'last_baoyang_date', 'last_baoyang_mile',\n",
    "                 'iterated_daydiff', 'iterated_dayofmile','iterated_milediff',\n",
    "                 'next_baoyang_date_fin', 'next_baoyang_mile_fin',\n",
    "                 'next_baoyang_date_fin_after_today','next_baoyang_mile_fin_after_today','iterated_id','method']\n",
    "    df=df.select(tongyi_cols).dropDuplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=get_two_dates(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.registerTempTable('pred_reg')\n",
    "\n",
    "spark.sql('drop table if exists clms.next_final_result')\n",
    "\n",
    "spark.sql('create table if not exists clms.next_final_result (vin string,purchase_date date,\\\n",
    "last_baoyang_date date,last_baoyang_mile double,iterated_daydiff double,iterated_dayofmile double,\\\n",
    "iterated_milediff double,next_baoyang_date_fin date,next_baoyang_mile_fin double,\\\n",
    "next_baoyang_date_fin_after_today date,next_baoyang_mile_fin_after_today double,iterated_id string,method string)')\n",
    "\n",
    "spark.sql('insert overwrite table clms.next_final_result select * from pred_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((time.time()-ft)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
